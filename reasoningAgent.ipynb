{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-07T01:15:49.140099Z",
     "start_time": "2025-12-07T01:15:49.134748Z"
    }
   },
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "from http.client import responses\n",
    "\n",
    "import requests\n",
    "from mpmath.math2 import math_sqrt\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")\n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")\n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.0,\n",
    "                                timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": 128,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ],
   "outputs": [],
   "execution_count": 300
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# self reflection :First inference method\n",
    "This was implemented in the agent loop"
   ],
   "id": "d37f7a339d330d78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Three of thought: Second inference method\n",
   "id": "7d091daa889ab338"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:15:49.148735Z",
     "start_time": "2025-12-07T01:15:49.143504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Node:\n",
    "    def __init__(self, thought, children = None):\n",
    "        self.thought = thought\n",
    "        self.children = children or []\n",
    "\n",
    "class Tot:\n",
    "    def __init__(self, prompt, model = None, iterations = 3, maxTokens = 128, treeOutput = None):\n",
    "        self.root = Node(prompt)\n",
    "        self.model = model or MODEL\n",
    "        self.iterations = iterations\n",
    "        self.current_thoughts = [self.root]\n",
    "        self.maxTokens = maxTokens\n",
    "        self.treeOutput = \"\"\n",
    "\n",
    "\n",
    "    def EThoughts(self, thoughtN):\n",
    "        newThoughtNodes = []\n",
    "\n",
    "        for nd in thoughtN:\n",
    "            prompt = f\"Given the current thought: '{nd.thought}', provide three concise thoughts that expand this idea further.\"\n",
    "            response = call_model_chat_completions(prompt)\n",
    "\n",
    "            if response:\n",
    "                child = Node(response[\"text\"])\n",
    "                nd.children.append(child)\n",
    "                newThoughtNodes.append(child)\n",
    "\n",
    "        return newThoughtNodes\n",
    "\n",
    "\n",
    "    def Run(self):\n",
    "        iteration = 0\n",
    "        while self.current_thoughts and iteration < self.iterations:\n",
    "            #print(f\"Iteration {iteration + 1}:\")\n",
    "            self.current_thoughts = self.EThoughts(self.current_thoughts)\n",
    "\n",
    "            # for tNode in self.current_thoughts:\n",
    "            #     print(f\"Explored Thought: {tNode.thought}\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "\n",
    "    def updateThought(self, nThought):\n",
    "        self.root = Node(nThought)\n",
    "        self.current_thoughts = [self.root]\n",
    "\n",
    "\n",
    "    def showTree(self, Node, lv=0, prefix=\"\",buffer = None):\n",
    "\n",
    "        # Reset buffer only on first/top-level call\n",
    "        if lv == 0:\n",
    "            self.treeOutput = \"\"\n",
    "\n",
    "        connector = \"└── \" if lv > 0 else \"\"\n",
    "        #print(f\"{prefix}{connector}{Node.thought}\")\n",
    "        line = f\"{prefix}{connector}{Node.thought}\"\n",
    "\n",
    "        self.treeOutput += line + \"\\n\"\n",
    "\n",
    "        # Prepare indentation for children\n",
    "        child_prefix = prefix + (\"    \" if lv > 0 else \"\")\n",
    "\n",
    "        for child in Node.children:\n",
    "            self.showTree(child, lv + 1, child_prefix)\n",
    "\n",
    "    def selectAnswer(self, original_prompt: str):\n",
    "\n",
    "        self.showTree(self.root)\n",
    "\n",
    "        ## USE FOR TESTING\n",
    "        #print(\"Printing the ogPrompt\",original_prompt)\n",
    "        answer = call_model_chat_completions(original_prompt ,system= f\"You are given a list of thoughts to help you decide on the answer look at the prompt and select branch with the most nodes. respond with just the answer. Attached below is the train of thought {self.treeOutput}\")\n",
    "\n",
    "        #print(\"the final answer is\", answer[\"text\"])\n",
    "        #print(\"The tree out is\", self.treeOutput)\n",
    "\n",
    "        return answer[\"text\"]\n",
    "\n"
   ],
   "id": "f5c81dd678f4e031",
   "outputs": [],
   "execution_count": 301
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Self-consistency : Third inference method\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2cb1f4f19c41211f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:15:49.155612Z",
     "start_time": "2025-12-07T01:15:49.151877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def determineProblem(prompt):\n",
    "    problemType = call_model_chat_completions(prompt, system= \"Determine what type of problem this is and return one of the following choices \"\n",
    "                                                              \"[Math reasoning, common sense, logic]\")\n",
    "    return problemType[\"text\"]\n",
    "\n",
    "def selfConsistency(prompt, num_responses = 5):\n",
    "    responses = []\n",
    "    for _ in range(num_responses):\n",
    "\n",
    "        ## funny way to rem temperature: When ur girlfriend is hot\"mad\" she is going to say a lot more and complain. When she is cold she will be relaxed and say less amen\n",
    "        #Before solving Response{_} must be written down\n",
    "        response = call_model_chat_completions(prompt, system = f\"\", temperature= .8)\n",
    "        responses.append(response[\"text\"])\n",
    "\n",
    "    formatFinalAnswers = responses\n",
    "    #formatFinalAnswers  = \"\\n---\\n\".join([f\"{key}:\\n{value}\" for key, value in responses])\n",
    "\n",
    "    #print(\"The formatFinal answers\",formatFinalAnswers)\n",
    "    selectionPrompt = f\"\"\"\n",
    "\n",
    "    I have generated several responses for the given problem. Please evaluate them and determine which is the most accurate, comprehensive, and well written\"\n",
    "\n",
    "    Here are the responses:\n",
    "    {formatFinalAnswers}\n",
    "\n",
    "    Review all candidate responses, determine which one is the strongest.\n",
    "    Return the answer after the Response: Example 'Response0: Yes'\n",
    "    Return would be \"yes\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    goldenSolution = call_model_chat_completions(selectionPrompt)\n",
    "    #print(\"The golden solution: \", goldenSolution) USE FOR TESTING\n",
    "\n",
    "    return goldenSolution[\"text\"], formatFinalAnswers\n",
    "\n",
    "\n",
    "\n",
    "def runSelfConsistency(prompt):\n",
    "    #problemType = determineProblem(prompt)\n",
    "    #print(problemType)\n",
    "    Answers = selfConsistency(prompt, num_responses= 5)\n",
    "    return Answers[0]\n",
    "    print(\"The golden solution is\", Answers[0], \"\\n\")\n",
    "    # for i in Answers[1]:\n",
    "    #     print(i)\n",
    "\n",
    "\n",
    "# prompt = \"For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials. It is always possible to model forces as being due to what?\"\n",
    "# runSelfConsistency(prompt)\n",
    "\n",
    "#\n",
    "# prompt = \"if there are three cars in the parking lot and 289 more cars arrive, how many cars are in the parking lot\"\n",
    "# problemType = determineProblem(prompt)\n",
    "# print(problemType)\n",
    "#\n",
    "# Answers = selfConsistency(prompt, num_responses= 5)\n",
    "# #print(Answers[0])\n",
    "#\n",
    "#\n",
    "# print(\"The golden solution is\", Answers[0])\n",
    "# for i in Answers[1]:\n",
    "#     print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9e9dfc7f8604a0a2",
   "outputs": [],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Agent loop\n",
    "\n",
    "Action route:\n",
    "1. Math or logic problems: self-consistency\n",
    "2. complex problems strategic planning, exploration, and backtracking: tree of thought\n",
    "3. Common sense"
   ],
   "id": "c701bebbb168dce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:15:49.164837Z",
     "start_time": "2025-12-07T01:15:49.158398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class agent:\n",
    "    #actionList =  [\"tot\", \"Consistency\", \"reflection\" ]\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "\n",
    "        self.action_list = [\"tot\", \"Consistency\", \"reflection\" ]\n",
    "        self.problem_types = [\"Math reasoning\", \"Common sense\", \"logic\"]\n",
    "\n",
    "    # def reason(self, context):\n",
    "    #     #print(\"working\")\n",
    "\n",
    "    def reflection(self, prompt: str):\n",
    "        #print(\"yes\")\n",
    "\n",
    "        # tot_agent = Tot(prompt)\n",
    "        # tot_agent.Run()\n",
    "        # tot_agent.showTree(tot_agent.root)\n",
    "        # tot_agent.selectAnswer(prompt)\n",
    "\n",
    "        oringialAnswer = runSelfConsistency(prompt)\n",
    "\n",
    "        reflectionPrompt = f\"\"\"\n",
    "        You are a critic and solver.\n",
    "\n",
    "        here is the original question:\n",
    "        {prompt}\n",
    "\n",
    "        here is an initial attempt at the solution\n",
    "        {oringialAnswer}\n",
    "\n",
    "        2. Provide only the final answer after correction.\n",
    "        \"\"\"\n",
    "\n",
    "        system_reflect = (\n",
    "        \"You are a strict but helpful reviewer. \"\n",
    "        \"Make sure the output is only the FINAL ANSWER.\"\n",
    "        )\n",
    "\n",
    "        refined = call_model_chat_completions(\n",
    "            reflectionPrompt,\n",
    "            system=system_reflect\n",
    "        )\n",
    "\n",
    "        if not refined or \"text\" not in refined:\n",
    "            return oringialAnswer\n",
    "\n",
    "\n",
    "        return refined[\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "    def planAction(self, prompt: str):\n",
    "\n",
    "        ##example\n",
    "        tests = [\n",
    "            {\n",
    "                \"id\": \"math_inequality\",\n",
    "                \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "                \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "                \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"commonsense_ice\",\n",
    "                \"type\": \"text\",\n",
    "                \"prompt\": (\n",
    "                    \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "                    \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "                    \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "                ),\n",
    "                \"expected\": \"stay the same\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"logic_race\",\n",
    "                \"type\": \"text\",\n",
    "                \"prompt\": (\n",
    "                    \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "                    \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "                ),\n",
    "                \"expected\": \"second\",\n",
    "            },\n",
    "            ]\n",
    "        systemPrompt = f\"\"\"\n",
    "        Determine what type of problem this is and return one of the following choices \"[Math reasoning, Common sense, logic]\"\n",
    "        attached below are examples of each problem type. When returning the answer just return the text\n",
    "        {tests}\n",
    "        \"\"\"\n",
    "        #action =  [\"tot\", \"Consistency\", \"reflection\" ]\n",
    "        #ProblemTypes = [\"Math reasoning\", \"Common sense\", \"logic\"]\n",
    "\n",
    "        DefineProblem = call_model_chat_completions(prompt, system= systemPrompt)\n",
    "        #DefineProblem = call_model_chat_completions(prompt, system=systemPrompt)\n",
    "\n",
    "        raw_text = (DefineProblem or {}).get(\"text\")\n",
    "\n",
    "        if not raw_text:\n",
    "            return self.action_list[1]\n",
    "\n",
    "        label = raw_text.strip()\n",
    "        if not label:\n",
    "            return self.action_list[1]\n",
    "\n",
    "\n",
    "        if not DefineProblem or \"text\" not in DefineProblem:\n",
    "            #print(\"Could not classify problem, defaulting to Consistency.\")\n",
    "            return self.action_list[1]  # default to self-consistency\n",
    "            #return action[1]\n",
    "\n",
    "        label = DefineProblem[\"text\"].strip()\n",
    "\n",
    "        ## Math Tree of Thought\n",
    "        if label == self.problem_types[0]:\n",
    "            #print(\"it is\" ,DefineProblem[\"text\"])\n",
    "            return self.action_list[0]\n",
    "\n",
    "         ## common sense Self-Consistency\n",
    "        elif label == self.problem_types[1]:\n",
    "            return self.action_list[1]\n",
    "\n",
    "        ## logic puzzles\n",
    "        elif label == self.problem_types[2]:\n",
    "           # print(\"it is\" ,DefineProblem[\"text\"])\n",
    "            return self.action_list[2]\n",
    "\n",
    "        return self.action_list[1]\n",
    "\n",
    "\n",
    "    def executeAction(self, action: str, prompt: str):\n",
    "        actionList =  [\"tot\", \"Consistency\", \"reflection\" ]\n",
    "        if action == actionList[0]:\n",
    "            #print(\"\\nPreforming Train of thought\")\n",
    "            tot_agent = Tot(prompt)\n",
    "            tot_agent.Run()\n",
    "            tot_agent.showTree(tot_agent.root)\n",
    "            return tot_agent.selectAnswer(prompt)\n",
    "\n",
    "        elif action == actionList[1]:\n",
    "            #print(\"\\npreforming consistency\")\n",
    "            return runSelfConsistency(prompt)\n",
    "\n",
    "        elif action == actionList[2]:\n",
    "            #print(\"\\npreforming reflection\")\n",
    "            return self.reflection(prompt)\n",
    "            #return runSelfConsistency(prompt)\n",
    "\n",
    "\n",
    "        else:\n",
    "            #print(\"Unknown action:\", action)\n",
    "            return None\n",
    "\n",
    "    def runAgent(self, task):\n",
    "        # Extract prompt from task\n",
    "        if isinstance(task, dict):\n",
    "            prompt = task.get(\"input\") or task.get(\"prompt\")\n",
    "        else:\n",
    "            prompt = str(task)\n",
    "\n",
    "        if not prompt:\n",
    "            raise ValueError(\"runAgent: No prompt found in task.\")\n",
    "\n",
    "        # reasoning context\n",
    "        # self.reason({\"task\": task})\n",
    "\n",
    "        # Decide which technique to use\n",
    "        action = self.planAction(prompt)\n",
    "        #print(\"Chosen action:\", action)\n",
    "\n",
    "        if action is None:\n",
    "            action = self.action_list[1]  # \"Consistency\"\n",
    "\n",
    "        # Execute the technique\n",
    "        answer = self.executeAction(action, prompt)\n",
    "\n",
    "        if not isinstance(answer, str) or not answer.strip():\n",
    "            answer = \"NO_ANSWER_GENERATED_DUE_TO_API_OR_CONNECTION_ERROR\"\n",
    "\n",
    "\n",
    "        answer = call_model_chat_completions(f\"The answer is {answer}\", system= \"Return ONLY the essential answer no explanation needed\")\n",
    "        #print(answer[\"text\"]) TESTING\n",
    "        return answer[\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#      agent = agent()\n",
    "#\n",
    "#      task = {\n",
    "#          \"input\": \"Is it possible to get killed walking to the Very Large Telescope? Facts: The Very Large Telescope is in the Atacama Desert The Atacama Desert is the driest hot desert in the world.\"\n",
    "#     }\n",
    "#\n",
    "#      result = agent.runAgent(task)\n",
    "#      print(\"Final answer:\", result)\n",
    "\n",
    "\n",
    "# A1 = agent\n",
    "# Action = A1.planAction(\"You place an ice cube in a glass of water and mark the water level. \"\n",
    "#             \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "#             \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\")\n",
    "# A1.executeAction(Action)\n",
    "\n"
   ],
   "id": "c5e089ff862d2591",
   "outputs": [],
   "execution_count": 303
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final testing: Answer generation",
   "id": "844a7dafad89a1e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:26:36.193173Z",
     "start_time": "2025-12-07T01:15:49.168155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Generate a placeholder answer file that matches the expected auto-grader format.\n",
    "\n",
    "Replace the placeholder logic inside `build_answers()` with your own agent loop\n",
    "before submitting so the ``output`` fields contain your real predictions.\n",
    "\n",
    "Reads the input questions from cse_476_final_project_test_data.json and writes\n",
    "an answers JSON file where each entry contains a string under the \"output\" key.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "INPUT_PATH = Path(\"first50Inputs.json\")\n",
    "OUTPUT_PATH = Path(\"first50_Outputs.json\")\n",
    "\n",
    "# INPUT_PATH = Path(\"question.json\")\n",
    "# OUTPUT_PATH = Path(\"answers.json\")\n",
    "\n",
    "def load_questions(path: Path) -> List[Dict[str, Any]]:\n",
    "    with path.open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Input file must contain a list of question objects.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_answers(questions: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    answers = []\n",
    "    count = 0\n",
    "    my_agent = agent()   # create a single agent instance\n",
    "    for idx, question in enumerate(questions, start=1):\n",
    "        # Example: assume you have an agent loop that produces an answer string.\n",
    "        # result = call_model_chat_completions(question[\"input\"])\n",
    "        # answers.append({\"output\": result[\"text\"]})\n",
    "        result = my_agent.runAgent(question)\n",
    "        answers.append({\"output\": result})\n",
    "\n",
    "        count += 1\n",
    "        if count == 100:\n",
    "            count = 0\n",
    "            print(idx, \"\\n\")\n",
    "\n",
    "        # real_answer = agent_loop(question[\"input\"])\n",
    "        # answers.append({\"output\": real_answer})\n",
    "\n",
    "        # placeholder_answer = f\"Placeholder answer for question {idx}\"\n",
    "        # answers.append({\"output\": placeholder_answer})\n",
    "    return answers\n",
    "\n",
    "\n",
    "def validate_results(\n",
    "    questions: List[Dict[str, Any]], answers: List[Dict[str, Any]]\n",
    ") -> None:\n",
    "    if len(questions) != len(answers):\n",
    "        raise ValueError(\n",
    "            f\"Mismatched lengths: {len(questions)} questions vs {len(answers)} answers.\"\n",
    "        )\n",
    "    for idx, answer in enumerate(answers):\n",
    "        if \"output\" not in answer:\n",
    "            raise ValueError(f\"Missing 'output' field for answer index {idx}.\")\n",
    "        if not isinstance(answer[\"output\"], str):\n",
    "            raise TypeError(\n",
    "                f\"Answer at index {idx} has non-string output: {type(answer['output'])}\"\n",
    "            )\n",
    "        if len(answer[\"output\"]) >= 5000:\n",
    "            raise ValueError(\n",
    "                f\"Answer at index {idx} exceeds 5000 characters \"\n",
    "                f\"({len(answer['output'])} chars). Please make sure your answer does not include any intermediate results.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    questions = load_questions(INPUT_PATH)\n",
    "    answers = build_answers(questions)\n",
    "\n",
    "    with OUTPUT_PATH.open(\"w\") as fp:\n",
    "        json.dump(answers, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with OUTPUT_PATH.open(\"r\") as fp:\n",
    "        saved_answers = json.load(fp)\n",
    "    validate_results(questions, saved_answers)\n",
    "    print(\n",
    "        f\"Wrote {len(answers)} answers to {OUTPUT_PATH} \"\n",
    "        \"and validated format successfully.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ],
   "id": "ac12cd35cf91a41a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 50 answers to first50_Outputs.json and validated format successfully.\n"
     ]
    }
   ],
   "execution_count": 304
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
